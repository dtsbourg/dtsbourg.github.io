<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>dtsbourg</title>
 <link href="http://dtsbourg.github.io/atom.xml" rel="self"/>
 <link href="http://dtsbourg.github.io/"/>
 <updated>2016-03-08T17:15:41+01:00</updated>
 <id>http://dtsbourg.github.io</id>
 <author>
   <name>Dylan Bourgeois</name>
   <email>dtsbourg@gmail.com</email>
 </author>

 
 <entry>
   <title>Free-fall recovery</title>
   <link href="http://dtsbourg.github.io/2015/12/14/free-fall-recovery-intro/"/>
   <updated>2015-12-14T00:00:00+01:00</updated>
   <id>http://dtsbourg.github.io/2015/12/14/free-fall-recovery-intro</id>
   <content type="html">&lt;p&gt;An account of my internship at &lt;a href=&quot;http://lis.epfl.ch/&quot;&gt;LIS&lt;/a&gt;. You can find most of the code
on &lt;a href=&quot;https://github.com/lis-epfl/MAVRIC_Library/tree/throw-recovery&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;http://rpg.ifi.uzh.ch/aggressive_flight.html&quot;&gt;RPG&lt;/a&gt; group from ETHZ published a paper titled &lt;em&gt;Automatic
Re-Initialization and Failure Recovery for Aggressive Flight with a Monocular
Vision-Based Quadrotor&lt;/em&gt; &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Their method would allow for the quadcopter to recover
from an arbitrary initial position, orientation and acceleration, which would be
useful as an emergency recovery method or as a fun way to launch your quad
by just throwing it in the air.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/pGU1s6Y55JI&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; class=&quot;center-image&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;br /&gt;
The main attraction of this method was that it did not rely on any external
sensing (for state estimation) or computation, leaving the quadcopter to carry
all of the computational load, allowing full autonomy and setting independence
(no need to map your surroundings beforehand).
This also means it can be used indoors as well as outdoors. Many of the current
methods used for automatic recovery rely on advanced positional tracking, most
commonly with infra-red sensors in a confined environment, like &lt;a href=&quot;http://www.optitrack.com&quot;&gt;Optitrack&lt;/a&gt; for example.&lt;/p&gt;

&lt;p&gt;We set out to implement a variation of their method, adapted to the &lt;a href=&quot;https://github.com/lis-epfl/MAVRIC_Library&quot;&gt;MAVRIC&lt;/a&gt; platform.
Their are several major differences in our work, most notably the fact that we have
not implemented optical tracking for position locking, the last stabilization stage.&lt;/p&gt;

&lt;h2 id=&quot;implementation-outline&quot;&gt;Implementation outline&lt;/h2&gt;

&lt;p&gt;The entire process is governed by a state machine that controls the transitions
between stabilization states based on given predicates or thresholds. This state
machine does have some specificities : it is organized somewhat as a cascade, where
once a task is considered accomplished, it does not end all together. During the next
steps, the previous stabilization mechanisms are still in effect. For example,
once we have stabilized our vertical velocity, we do not want our quad to fall like
a rock if our new goal is to stabilize our horizontal velocity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/modules.svg&quot; alt=&quot;  &quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;center&gt;&lt;strong&gt;Figure 1 : Stabilization phases&lt;/strong&gt;&lt;/center&gt;

&lt;p&gt;The module structure is shown above. Let’s go through it quickly. Some of these
will be described more in detail in following posts.&lt;/p&gt;

&lt;h3 id=&quot;launch-detection&quot;&gt;Launch detection&lt;/h3&gt;
&lt;p&gt;First of all, we need to find out if our quad has been thrown or it is falling. To do that, we use
the fact that an object in free-fall will feel no acceleration as it reaches the
apex of its trajectory. In the real world, sensors are noisy, outside forces do
apply (one problematic one is the centrifugal force : if the quad starts rotating
around its own axis, it will feel an extra acceleration on top of that caused by gravity), so we
can’t rely on a 0-valued instantaneous measurement form the accelerometers. We will define
a threshold under which we can assume the object is falling. Also, we will be looking
at a mean value over a given number of samples to make sure we’re actually seeing
what we want to see and not just noise : this is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Moving_average&quot;&gt;moving average&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/sma_pres_big.png&quot; alt=&quot;  &quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;center&gt;&lt;strong&gt;Figure 2 : Acceleration measured over time (in ms)&lt;/strong&gt;&lt;/center&gt;

&lt;p&gt;As you will notice in the above the figure, there is a large peak when the quad is thrown,
then a drop as we reach free-fall, but we are still far from reaching a zero-value.
Once the threshold is crossed, we can start off the second phase : attitude control.&lt;/p&gt;

&lt;h3 id=&quot;attitude-control&quot;&gt;Attitude control&lt;/h3&gt;

&lt;p&gt;To define where a quadcopter is in space, you need your regular &lt;em&gt;x&lt;/em&gt;, &lt;em&gt;y&lt;/em&gt;, &lt;em&gt;z&lt;/em&gt; coordinates
(or whatever coordinate system you chose). You now need to figure out how to express
your quad’s orientation in space. For that, three angles are used : &lt;em&gt;yaw&lt;/em&gt;, &lt;em&gt;pitch&lt;/em&gt; and &lt;em&gt;roll&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/attitude.jpg&quot; alt=&quot;  &quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;strong&gt;Figure 3 : A quad with an attitude [2] &lt;/strong&gt;&lt;/center&gt;

&lt;p&gt;The first stabilization we need to accomplish is to get the quad back upright :
we need roll and pitch angles to be 0 so we can be horizontal with respect to
the ground level.&lt;/p&gt;

&lt;h3 id=&quot;vertical-velocity&quot;&gt;Vertical velocity&lt;/h3&gt;

&lt;p&gt;Now that we’re parallel to the ground, we need to avoid hitting it head on. Just
by adding some thrust to the rotors, we can keep the quad from hitting the ground.
The target vertical speed would ideally be &lt;em&gt;0 m/s&lt;/em&gt;, but we can start the next phase
even with a small vertical velocity (upwards or downwards) and the remainder of the
error will be canceled out during the next phases of our stabilization (remember the
cascade nature of our state machine).&lt;/p&gt;

&lt;h3 id=&quot;height-control&quot;&gt;Height control&lt;/h3&gt;

&lt;p&gt;Once we know we’re not shooting for the ground, we have to get a sense of where we
are in space, particularly (and most urgently) our height. If we took too much time
to stabilize our vertical speed we might be very close to the floor. We can decide
arbitrarily how high we want to go, let’s settle for &lt;em&gt;2m&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Now how does our quad find out where it is ? Well it has to sense it, using … sensors.
Yeah, I know pretty boring ellipsis … The interesting part is that it doesn’t use
only one sensor but rather a subtle mix of different information from different sensors called
MSF, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Sensor_fusion&quot;&gt;Multi-Sensor Fusion&lt;/a&gt;.
This technique is used throughout our project and is an easy way to have the most
accurate estimate possible, essential to have a functional control algorithm. For
the altitude we will use a combination of a sonar &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, our GPS and a barometer &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;horizontal-velocity&quot;&gt;Horizontal velocity&lt;/h3&gt;

&lt;p&gt;Okay, so we’ve avoided hitting the ground but what if there are obstacles on our
way ? We have to avoid drifting along once we’ve reached a desired altitude. The
same methods are used for this step as the vertical velocity, except this time we
can use the fact that our quadcopter has four rotors (it’s in the name !). By
combining different speeds on a combination of these motors we can move in different
directions : that’s how a quad is so agile. For example, putting more thrust on
the back propellers will incline the drone forward and make it move. We can use this
effect to our advantage in this step : if we’re drifting “back”, we just have to
force it to move forwards, cancelling the previous movement !&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/quadrotor_movement.jpg&quot; alt=&quot;  &quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;strong&gt;Figure 4 : How does a quadcopter get around ? [^5]&lt;/strong&gt;&lt;/center&gt;

&lt;h3 id=&quot;position-locking&quot;&gt;Position locking&lt;/h3&gt;

&lt;p&gt;Finally we’ve arrived at the last stabilization step. After setting the drone upright,
cancelling its original momentum in the vertical and horizontal planes and sticking
to a defined altitude we can now set its final resting place. We can assign the quad
to a given position and it will just sit there. Isn’t magic ? Okay, admittedly it’s
not that easy. As we mentioned we do not have ground truth (ie an absolute “true”
value for its actual position), we can only estimate where it actually is. So minimizing
errors is essential, but we can’t cancel them all together : reality is messy. We
can try our best to stick to our position, once again playing with a combination
of thrusts to counteract any drift that may appear.&lt;/p&gt;

&lt;h2 id=&quot;preliminary-results&quot;&gt;Preliminary results&lt;/h2&gt;

&lt;p&gt;So, after all this, does it actually work ? Yeah ! We’ve been able to reproduce
the stabilization behavior described in the publication. Our implementation is
cross-platform so it can in theory be used anywhere our library runs. It needs
minimal configuration, and can be manually enabled or disabled from a remote control
switch.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://youtube.com/embed/2tp7x3Xg5Hk&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; class=&quot;center-image&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;br /&gt;
Trying to define how fast it is can be a tricky question, because some steps
strongly depend on the initial conditions, especially for height control. Here’s
a little recap of the average time it takes to complete each step, based on tests
carried out on our platform.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Step&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Launch detection&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;350 ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Attitude control&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;675 ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Vertical velocity&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;200-800 ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Height control&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Horizontal velocity&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;800-1500ms&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;One of the major drawbacks of our method is the use of a GPS, which restricts its
usage to outdoor configurations. Nevertheless it is not a technical limitation to
add an optical tracking method to the algorithm, for example with a &lt;a href=&quot;https://pixhawk.org/modules/px4flow&quot;&gt;PX4Flow&lt;/a&gt; sensor
or a simple laser motion sensor, like the ADNS 9800 &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; which is used in computer
mice. They use a very interesting method called &lt;a href=&quot;https://en.wikipedia.org/wiki/Optical_flow&quot;&gt;optical flow&lt;/a&gt;. The idea is that there
is no need for an optical sensor to have a high resolution or a acute knowledge
of what it’s looking at. Instead, we compute a difference between two successive
images, from which we can deduce a movement vector which tells us in what direction
and how fast we are moving.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/optical_flow.png&quot; alt=&quot;  &quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;&lt;strong&gt;Figure 5 : Optical flow method [^7]&lt;/strong&gt;&lt;/center&gt;

&lt;h2 id=&quot;wrapping-up&quot;&gt;Wrapping up&lt;/h2&gt;

&lt;p&gt;This is the first of a series of posts about this project, a high level introduction
to what the work accomplished was all about. I hope you stay tuned for some more
technical insight on how it actually works, but as you can already see, the elegance
of control theory allows for some impressive results using only very simple concepts.&lt;/p&gt;

&lt;p&gt;Cheers !&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;IEEE International Conference on Robotics and Automation (ICRA), Seattle, 2015, &lt;a href=&quot;http://rpg.ifi.uzh.ch/docs/ICRA15_Faessler.pdf&quot;&gt;Link&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Reference : MB1242 I2CXL Sonar, &lt;a href=&quot;http://www.maxbotix.com/documents/I2CXL-MaxSonar-EZ_Datasheet.pdf&quot;&gt;Datasheet&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;Reference : BMP085 Barometer, &lt;a href=&quot;https://www.adafruit.com/datasheets/BMP085_DataSheet_Rev.1.0_01July2008.pdf&quot;&gt;Datasheet&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;Reference : ADNS 9800 HD Optical Flow Sensor, &lt;a href=&quot;http://www.pixart.com.tw/upload/ADNS-9800%20DS_S_V1.0_20130514144352.pdf&quot;&gt;Datasheet&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Hello world !</title>
   <link href="http://dtsbourg.github.io/2015/08/01/hello-world/"/>
   <updated>2015-08-01T00:00:00+02:00</updated>
   <id>http://dtsbourg.github.io/2015/08/01/hello-world</id>
   <content type="html">&lt;p&gt;Welcome to dtsbourg.github.io. It’s quite empty for now but hopefully lots of stuff coming :) Stay tuned !&lt;/p&gt;
</content>
 </entry>
 

</feed>
